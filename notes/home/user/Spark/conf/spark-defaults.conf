############################
# 基本运行模式
############################
# 使用 YARN 作为资源管理器
spark.master                       yarn
# client：Driver 在提交节点（适合学习 / 调试）
# cluster：Driver 在 YARN（适合生产）
spark.submit.deployMode            client

############################
# 应用 & Web UI
############################
# Spark 应用默认名称
spark.app.name                     SparkOnYarnApp
# Driver Web UI 端口
spark.ui.port                      4040
# 是否启用 Spark Web UI
spark.ui.enabled                   true

############################
# 事件日志（强烈推荐）
############################
# 开启事件日志（历史服务器用）
spark.eventLog.enabled             true
# 日志存储位置（HDFS）
spark.eventLog.dir                 hdfs:///spark-logs
# 是否压缩事件日志
spark.eventLog.compress            true

############################
# Driver 资源配置
############################
# Driver 使用的内存大小
spark.driver.memory                768m
spark.yarn.driver.memoryOverhead   256m
# Driver 使用的 CPU 核数
spark.driver.cores                 1
# Driver 最大结果集大小（防 OOM）
spark.driver.maxResultSize         512m

############################
# Executor 资源配置
############################
# Executor 使用的内存
spark.executor.memory              1g
spark.yarn.executor.memoryOverhead 256m
# 每个 Executor 使用的 CPU 核数
spark.executor.cores               1
# Executor 数量（静态方式）
spark.executor.instances           2

############################
# 序列化 & 性能
############################
# 使用 Kryo（比 Java 快）
spark.serializer                   org.apache.spark.serializer.KryoSerializer
# Kryo 缓冲区大小
spark.kryoserializer.buffer        32k
spark.kryoserializer.buffer.max    64m

############################
# Shuffle & 网络
############################
# Shuffle 分区数（默认 200 太大）
spark.sql.shuffle.partitions       4
# 是否启用 shuffle 压缩
spark.shuffle.compress             true
# 是否启用 spill 压缩
spark.shuffle.spill.compress       true

############################
# 内存管理
############################
# 统一内存管理
spark.memory.fraction              0.6
spark.memory.storageFraction       0.5

############################
# 容错 & 重试
############################
# Task 失败最大重试次数
spark.task.maxFailures             4
# Stage 重试次数
spark.stage.maxConsecutiveAttempts 4
# 网络超时
spark.network.timeout              300s
